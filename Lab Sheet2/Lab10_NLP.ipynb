{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. Download the pre-trained models for the following word\n",
    "embedding models - check the notebooks uploaded.\n",
    "a. GloVe\n",
    "b. Word2Vec\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "# GloVe\n",
    "\n",
    "# Download the GloVe model\n",
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "\n",
    "# Unzip the GloVe model\n",
    "!unzip glove.6B.zip\n",
    "\n",
    "# Word2Vec\n",
    "\n",
    "# Download the Word2Vec model\n",
    "!wget https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
    "\n",
    "# Unzip the Word2Vec model\n",
    "!gunzip GoogleNews-vectors-negative300.bin.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2. Create document vectors by the following formula:\n",
    "where:\n",
    "a. doc_Veci : ith document in the corpus.\n",
    "b. wj : word vector of j\n",
    "th word in the document. The word vector is\n",
    "taken model.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import FastText\n",
    "\n",
    "\n",
    "# Load the GloVe model\n",
    "glove_model = KeyedVectors.load_word2vec_format('glove.6B.300d.txt', binary=False)\n",
    "\n",
    "# Load the Word2Vec model\n",
    "word2vec_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "# Load the FastText model\n",
    "fasttext_model = FastText.load_fasttext_format('wiki.en.bin')\n",
    "\n",
    "# Load the corpus\n",
    "corpus = pd.read_csv('corpus.csv')\n",
    "\n",
    "# Preprocess the corpus\n",
    "def preprocess(text):\n",
    "    # Convert to lower case\n",
    "    text = text.lower()\n",
    "    # Tokenize the text\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    # Remove the stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # Remove the punctuations\n",
    "    tokens = [w for w in tokens if w.isalpha()]\n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens]\n",
    "    # Stem the tokens\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(w) for w in tokens]\n",
    "    return tokens\n",
    "\n",
    "# Create the document vectors\n",
    "def create_doc_vectors(model, corpus):\n",
    "    # Preprocess the corpus\n",
    "    corpus['tokens'] = corpus['text'].apply(preprocess)\n",
    "    # Create the document vectors\n",
    "    doc_vectors = []\n",
    "    for i in range(len(corpus)):\n",
    "        doc_vector = np.zeros(300)\n",
    "        for j in range(len(corpus['tokens'][i])):\n",
    "            try:\n",
    "                doc_vector += model[corpus['tokens'][i][j]]\n",
    "            except:\n",
    "                pass\n",
    "        doc_vectors.append(doc_vector)\n",
    "    return doc_vectors\n",
    "\n",
    "# Create the document vectors for the GloVe model\n",
    "glove_doc_vectors = create_doc_vectors(glove_model, corpus)\n",
    "\n",
    "# Create the document vectors for the Word2Vec model\n",
    "word2vec_doc_vectors = create_doc_vectors(word2vec_model, corpus)\n",
    "\n",
    "# Create the document vectors for the FastText model\n",
    "fasttext_doc_vectors = create_doc_vectors(fasttext_model, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4fc14cfe266f72e2b60b8f11f2f57377376f5ef662aa88be7ba3ee6e3e3df6cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

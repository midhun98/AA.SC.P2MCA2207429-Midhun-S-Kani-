{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Load the text in data.txt to a variable text. Apply lower casing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Natural language processing makes it possible for computers\n",
    "to understand the human language. In natural language\n",
    "processing, human language is separated into fragments so that\n",
    "the grammatical structure of sentences and the meaning of words\n",
    "can be analysed and understood in context. This helps computers\n",
    "read and understand spoken or written text in the same way as\n",
    "humans. I am studying Natural Language Processing at Amrita\n",
    "University.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordTokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitTokens = text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n",
      "67\n"
     ]
    }
   ],
   "source": [
    "print(len(wordTokens))\n",
    "print(len(splitTokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.', ',', 'context', '.', 'humans', '.', 'university', '.']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing = [x for x in wordTokens if x not in splitTokens]\n",
    "missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['language.', 'processing,', 'context.', 'humans.', 'university.']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing2 = [x for x in splitTokens if x not in wordTokens]\n",
    "missing2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 a). How is str.split() different from word tokenizer?  \n",
    "\n",
    "\n",
    "str.split() -- splits the words by empty spaces and not by special character or anything  \n",
    "word_tokenize -- splits the sentence by spaces and by special characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Apply the sentence tokenization process to the text and store in\n",
    "sentTokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natural language processing makes it possible for computers\\nto understand the human language.',\n",
       " 'in natural language\\nprocessing, human language is separated into fragments so that\\nthe grammatical structure of sentences and the meaning of words\\ncan be analysed and understood in context.',\n",
       " 'this helps computers\\nread and understand spoken or written text in the same way as\\nhumans.',\n",
       " 'i am studying natural language processing at amrita\\nuniversity.']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentTokens = sent_tokenize(text)\n",
    "sentTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural language processing makes it possible for computers\\n', 'to understand the human language. In natural language\\n', 'processing, human language is separated into fragments so that\\n', 'the grammatical structure of sentences and the meaning of words\\n', 'can be analysed and understood in context. This helps computers\\n', 'read and understand spoken or written text in the same way as\\n', 'humans. I am studying Natural Language Processing at Amrita\\n', 'University.\\n']\n"
     ]
    }
   ],
   "source": [
    "file1 = open('sentence', 'r')\n",
    "Lines = file1.readlines()\n",
    "print(Lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. How is readlines() different from sentence tokenizer.  \n",
    "b. How many tokens are there now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "print(len(sentTokens))\n",
    "print(len(Lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Print the tokens and compare them with the readlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natural language processing makes it possible for computers\\nto understand the human language.',\n",
       " 'in natural language\\nprocessing, human language is separated into fragments so that\\nthe grammatical structure of sentences and the meaning of words\\ncan be analysed and understood in context.',\n",
       " 'this helps computers\\nread and understand spoken or written text in the same way as\\nhumans.',\n",
       " 'i am studying natural language processing at amrita\\nuniversity.']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing2 = [x for x in sentTokens if x not in Lines]\n",
    "missing2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Apply spelling correction on each word tokens and print the initial 15\n",
    "misspelled tokens as well as the corrected tokens. Keep a count of\n",
    "corrected tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Remove stop words and punctation characters from the corrected token list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "StopWords = stopwords.words(\"english\")\n",
    "no_stopwords = [x for x in wordTokens if x not in StopWords]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Stem each tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(i) for i in wordTokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
